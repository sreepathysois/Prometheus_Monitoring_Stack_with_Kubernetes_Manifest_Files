###### Intsall prometheus operator and all CRD"""


kubectl create -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/master/bundle.yaml


####create file called prom_rbac.yam#########

# prom_rbac.yaml (fixed)
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
rules:
- apiGroups: [""]
  resources:
  - nodes
  - nodes/metrics
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["get"]
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  verbs: ["get", "list", "watch"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: default



kubectl apply -f prom_rbac.yaml  



#####crete prometheus statefullset resource prometheus.yaml####



apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: prometheus
  labels:
    app: prometheus
spec:
  image: quay.io/prometheus/prometheus:v2.55.1
  nodeSelector:
    kubernetes.io/os: linux
  replicas: 2
  resources:
    requests:
      memory: 400Mi
  securityContext:
    fsGroup: 2000
    runAsNonRoot: true
    runAsUser: 1000
  serviceAccountName: prometheus
  version: v2.22.1
  serviceMonitorSelector: {}
  serviceMonitorNamespaceSelector: {}
  ruleSelector: {}
  ruleNamespaceSelector: {}
  alerting:
    alertmanagers:
      - namespace: default
        name: alertmanager
        port: web

kubectl apply -f prometheus.yaml 



####create prometheus service prometheus_svc.yaml####



apiVersion: v1
kind: Service
metadata:
  name: prometheus
  labels:
    app: prometheus
spec:
  ports:
  - name: web
    port: 9090
    targetPort: web
  selector:
    app.kubernetes.io/name: prometheus
  sessionAffinity: ClientIP


kubectl apply -f prometheus_svc.yaml


####Test application Exposing Metrics###########


sudo git clone https://github.com/sreepathysois/Nodej_Prometheus_Metric-Monitor.git

cd Nodej_Prometheus_Metric-Monitor


kubectl apply -f nodejs_prometheus_deployment.yaml

kubetcl apply -f nodejs_prometheus_service.yaml
 
kubetcl apply -f nodejs_prometheus_service_monitor.yaml  




Node.js App  →  Service  →  ServiceMonitor
     ↓                    ↑
  exports /metrics         \
       ↓                    \
     Prometheus ←────────── Prometheus Operator
       ↓
   Grafana (optional) 



####Deploy grafana grafana_deploy_svc.yaml#########



apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      containers:
        - name: grafana
          image: grafana/grafana:latest
          env:
            - name: GF_SECURITY_ADMIN_USER
              value: admin
            - name: GF_SECURITY_ADMIN_PASSWORD
              value: admin
          ports:
            - containerPort: 3000
              name: web
---
apiVersion: v1
kind: Service
metadata:
  name: grafana
spec:
  type: NodePort
  selector:
    app: grafana
  ports:
    - port: 3000
      targetPort: 3000
      nodePort: 30030
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-datasource
  labels:
    grafana_datasource: "1"
data:
  prometheus-datasource.yaml: |
    apiVersion: 1
    datasources:
      - name: Prometheus
        type: prometheus
        access: proxy
        url: http://prometheus:9090
        isDefault: true


kubetcl apply -f grafana_deploy_svc.yaml


####Prometheus pql####




🧠 Understanding Metric Types You Have

From your output, your app exports:

Metric Name	Type	Description
cafe_welcome_requests_total	counter	total number of welcome page hits
process_cpu_seconds_total	counter	total CPU time used (user+system)
process_resident_memory_bytes	gauge	current memory usage
nodejs_eventloop_lag_seconds	gauge	event loop lag (main thread delay)
nodejs_heap_size_used_bytes	gauge	heap memory currently used
nodejs_gc_duration_seconds	histogram	GC pause durations by type
nodejs_active_handles_total	gauge	active I/O handles
nodejs_active_resources_total	gauge	active event loop resources
nodejs_version_info	gauge	Node.js version metadata

Let’s query these step by step. 👇

⚙️ 1️⃣ Request Rate — How many welcome requests per second?
rate(cafe_welcome_requests_total[1m])


👉 Returns the requests per second over the last minute.
This is the most common way to track web endpoint traffic.

✅ In Grafana, you can visualize this as “Requests per second.”

You can also take a longer window (5m average):

rate(cafe_welcome_requests_total[5m])

⚙️ 2️⃣ Total Request Count
cafe_welcome_requests_total


This just shows the total number (e.g., 3).
Useful for verifying that your counter increments as you refresh the web page.

⚙️ 3️⃣ CPU Usage (seconds per second → percentage)

Prometheus exposes CPU time as a counter, so you need to rate it:

rate(process_cpu_seconds_total[1m])


Multiply by 100 for approximate CPU utilization (% of one core):

rate(process_cpu_seconds_total[1m]) * 100

⚙️ 4️⃣ Memory Usage (Resident / Heap)

Resident (RSS):

process_resident_memory_bytes


This shows total physical memory used (bytes).

Convert to MB:

process_resident_memory_bytes / 1024 / 1024


Heap Memory Used:

nodejs_heap_size_used_bytes / 1024 / 1024

⚙️ 5️⃣ Event Loop Lag — responsiveness indicator
nodejs_eventloop_lag_seconds


This tells you how blocked your Node.js main thread is.
Healthy apps usually stay below 0.02s (20ms).

⚙️ 6️⃣ Number of Active Handles / Resources

Handles:

nodejs_active_handles_total


Resources:

nodejs_active_resources_total


To see resource breakdown (by type):

nodejs_active_resources

⚙️ 7️⃣ Heap Space Usage by Space

For detailed breakdown (old, new, code, etc.):

Used bytes per heap space:

nodejs_heap_space_size_used_bytes


Percentage of heap used per space:

(nodejs_heap_space_size_used_bytes / nodejs_heap_space_size_total_bytes) * 100

⚙️ 8️⃣ Garbage Collection Metrics (Histogram)

Total GC count by type:

sum(nodejs_gc_duration_seconds_count) by (kind)


Total GC time spent (seconds):

sum(nodejs_gc_duration_seconds_sum) by (kind)


Average GC duration per GC type:

(sum(nodejs_gc_duration_seconds_sum) by (kind))
/
(sum(nodejs_gc_duration_seconds_count) by (kind))

⚙️ 9️⃣ Node.js Version
nodejs_version_info


→ returns {version="v18.20.8", major="18", minor="20", patch="8"} 1
A nice sanity check that metrics are coming from your running Node.js version.

⚙️ 🔟 Combine Metrics for Dashboards

Dashboard ideas for Grafana:

Chart	PromQL Query	Notes
Requests per second	rate(cafe_welcome_requests_total[1m])	Traffic graph
CPU usage %	rate(process_cpu_seconds_total[1m]) * 100	CPU load
Memory (MB)	process_resident_memory_bytes / 1024 / 1024	RAM used
Heap usage %	(nodejs_heap_size_used_bytes / nodejs_heap_size_total_bytes) * 100	Memory efficiency
Event loop lag	nodejs_eventloop_lag_seconds	Responsiveness
Active handles	nodejs_active_handles_total	Connections / async handles
GC avg duration	(sum(nodejs_gc_duration_seconds_sum) by (kind)) / (sum(nodejs_gc_duration_seconds_count) by (kind))  




#### Alert Manager and Alerts Setup#####



####Alertmanager rbac create file alertmanager_rbac.yaml####



# alertmanager-rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: alertmanager
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: alertmanager
rules:
- apiGroups: [""]
  resources:
  - configmaps
  - secrets
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: alertmanager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: alertmanager
subjects:
- kind: ServiceAccount
  name: alertmanager
  namespace: default



kubectl apply -f alertmanager_rbac.yaml



####create alert manager deploy file alertmanager_deploy.yaml #####



# alertmanager.yaml
apiVersion: monitoring.coreos.com/v1
kind: Alertmanager
metadata:
  name: alertmanager
  labels:
    app: alertmanager
spec:
  replicas: 1
  serviceAccountName: alertmanager
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000



kubectl apply -f alertmanager_deploy.yaml  



#### Create alert manager service alertmanager-svc.yam####


# alertmanager-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: alertmanager
  labels:
    app: alertmanager
spec:
  type: NodePort
  selector:
    alertmanager: alertmanager
  ports:
    - name: web
      port: 9093
      targetPort: 9093
      nodePort: 30093


kubectl apply -f alertmanager_svc.yaml


#### Create Alert Rules and Config Files and Test###########

### Create prometheus_alert_rules.yaml


apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: kubernetes-ai-alerts
  namespace: default
  labels:
    app: prometheus
    release: prometheus
spec:
  groups:
  - name: ai-kubernetes-alerts
    rules:
    - alert: PodImagePullError
      expr: kube_pod_container_status_waiting_reason{reason=~"ImagePullBackOff|ErrImagePull"} > 0
      for: 15s
      labels:
        severity: warning
        ai_alert: "true"
      annotations:
        summary: "Pod image pull error detected"
        description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} failed to pull image. Verify image name, tag, or registry credentials."

    - alert: CrashLoopBackOff
      expr: kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff"} > 0
      for: 15s
      labels:
        severity: warning
        ai_alert: "true"
      annotations:
        summary: "Pod in CrashLoopBackOff state"
        description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is restarting repeatedly. Check logs and resource limits."

    - alert: PodOOMKilled
      expr: kube_pod_container_status_terminated_reason{reason="OOMKilled"} > 0
      for: 1m
      labels:
        severity: warning
        ai_alert: "true"
      annotations:
        summary: "Container OOMKilled"
        description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} was OOMKilled. Consider increasing memory limits."

    - alert: HighCpuUsage
      expr: sum(rate(container_cpu_usage_seconds_total{image!=""}[2m])) by (pod, namespace) > 0.8
      for: 2m
      labels:
        severity: warning
        ai_alert: "true"
      annotations:
        summary: "High CPU usage detected"
        description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is using >80% CPU for 2 minutes."

    - alert: HighMemoryUsage
      expr: sum(container_memory_usage_bytes{image!=""}) by (pod, namespace)
        / sum(kube_pod_container_resource_limits_memory_bytes{image!=""}) by (pod, namespace) > 0.9
      for: 2m
      labels:
        severity: warning
        ai_alert: "true"
      annotations:
        summary: "High memory usage detected"
        description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is using >90% of its memory limit."

    - alert: NodeDown
      expr: up{job="node-exporter"} == 0
      for: 1m
      labels:
        severity: critical
        ai_alert: "true"
      annotations:
        summary: "Node down"
        description: "Node {{ $labels.instance }} is not responding."

    - alert: PodNetworkUnavailable
      expr: kube_pod_status_reason{reason="NetworkUnavailable"} > 0
      for: 1m
      labels:
        severity: warning
        ai_alert: "true"
      annotations:
        summary: "Pod network unavailable"
        description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has network connectivity issues."

    - alert: NodeDiskPressure
      expr: kube_node_status_condition{condition="DiskPressure", status="true"} == 1
      for: 2m
      labels:
        severity: warning
        ai_alert: "true"
      annotations:
        summary: "Node disk pressure"
        description: "Node {{ $labels.node }} has DiskPressure=true. Check disk space or I/O bottlenecks."

    - alert: PodPending
      expr: kube_pod_status_phase{phase="Pending"} > 0
      for: 3m
      labels:
        severity: warning
        ai_alert: "true"
      annotations:
        summary: "Pod pending too long"
        description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} pending >3m. Possible scheduling/resource issues."

    - alert: PodUnschedulable
      expr: kube_pod_status_unschedulable > 0
      for: 2m
      labels:
        severity: warning
        ai_alert: "true"
      annotations:
        summary: "Pod unschedulable"
        description: "Pod {{ $labels.pod }} cannot be scheduled due to resource constraints."


kubectl apply -f prometheus_alert_rules.yam



####create alert manager config  alertmanager_secret_config.yaml#####


# alertmanager_secret_config.yaml
apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-alertmanager
  namespace: default
  labels:
    app: alertmanager
type: Opaque
stringData:
  alertmanager.yaml: |-
    global:
      resolve_timeout: 5m

    route:
      group_by: ['alertname']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 3h
      receiver: 'discord'
      routes:
        - match:
            ai_alert: "true"
          receiver: 'discord'
        - match_re:
            severity: "critical|warning"
          receiver: 'discord'

    receivers:
      - name: 'discord'
        webhook_configs:
          - url: 'https://discord.com/api/webhooks/1234567890/abcdEfGhIjKlMnOpQrStUvWxYz'
            send_resolved: true

    templates:
      - '/etc/alertmanager/config/*.tmpl'



#### Install exportes to export metrucs kube state metrics, node exporter####


### create kube_state_metrics.yaml####



# kube-state-metrics.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kube-state-metrics
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: kube-state-metrics
rules:
  - apiGroups: [""]
    resources: ["pods", "nodes", "services", "endpoints", "daemonsets", "replicasets", "statefulsets"]
    verbs: ["list", "watch"]
  - apiGroups: ["apps"]
    resources: ["deployments", "daemonsets", "statefulsets", "replicasets"]
    verbs: ["list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kube-state-metrics
subjects:
  - kind: ServiceAccount
    name: kube-state-metrics
    namespace: default
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kube-state-metrics
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kube-state-metrics
  template:
    metadata:
      labels:
        app: kube-state-metrics
    spec:
      serviceAccountName: kube-state-metrics
      containers:
        - name: kube-state-metrics
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0
          ports:
            - name: metrics
              containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: kube-state-metrics
  labels:
    app: kube-state-metrics
spec:
  type: ClusterIP
  selector:
    app: kube-state-metrics
  ports:
    - name: metrics
      port: 8080
      targetPort: 8080


#### create node_exporter.yaml###


# node-exporter.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: node-exporter
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: node-exporter
rules:
  - apiGroups: [""]
    resources: ["nodes", "nodes/proxy", "services", "endpoints", "pods"]
    verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: node-exporter
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: node-exporter
subjects:
  - kind: ServiceAccount
    name: node-exporter
    namespace: default
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
spec:
  selector:
    matchLabels:
      app: node-exporter
  template:
    metadata:
      labels:
        app: node-exporter
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9100"
    spec:
      serviceAccountName: node-exporter
      hostNetwork: true
      hostPID: true
      containers:
        - name: node-exporter
          image: prom/node-exporter:latest
          ports:
            - name: metrics
              containerPort: 9100
          args:
            - --path.sysfs=/host/sys
            - --path.rootfs=/host/root
          volumeMounts:
            - name: sys
              mountPath: /host/sys
              readOnly: true
            - name: root
              mountPath: /host/root
              readOnly: true
      volumes:
        - name: sys
          hostPath:
            path: /sys
        - name: root
          hostPath:
            path: /
---
apiVersion: v1
kind: Service
metadata:
  name: node-exporter
  labels:
    app: node-exporter
spec:
  type: ClusterIP
  selector:
    app: node-exporter
  ports:
    - name: metrics
      port: 9100
      targetPort: 9100


#### Create Service Monitors for Kube state metrics and Node exporter metrics

### Create kube_state_metrics_service_monotor.yaml


apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: kube-state-metrics
  labels:
    release: prometheus
spec:
  selector:
    matchLabels:
      app: kube-state-metrics
  endpoints:
    - port: metrics
      interval: 15s


#### create node_exporter_service_monitor.yaml

apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: node-exporter
  labels:
    release: prometheus
spec:
  selector:
    matchLabels:
      app: node-exporter
  endpoints:
    - port: metrics
      interval: 15s





##### Pod Monitor#####


Create a Kubernetes Service for Kubelet endpoints

We need a Service that points to all node kubelets.
Let’s make it in the kube-system namespace:

# kubelet-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubelet
  namespace: kube-system
  labels:
    k8s-app: kubelet
spec:
  selector:
    component: kubelet
  ports:
    - name: https-metrics
      port: 10250
      targetPort: 10250


👉 This service helps Prometheus discover the kubelets on each node via Kubernetes API service discovery.

Apply it:

kubectl apply -f kubelet-service.yaml

⚙️ 4️⃣ Create a ServiceMonitor for kubelet + cAdvisor

Now add this ServiceMonitor so Prometheus can scrape /metrics and /metrics/cadvisor endpoints.

# kubelet-servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: kubelet
  namespace: kube-system
  labels:
    release: prometheus
spec:
  namespaceSelector:
    matchNames:
      - kube-system
  selector:
    matchLabels:
      k8s-app: kubelet
  endpoints:
    - port: https-metrics
      scheme: https
      path: /metrics
      interval: 30s
      tlsConfig:
        insecureSkipVerify: true
      bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    - port: https-metrics
      scheme: https
      path: /metrics/cadvisor
      interval: 30s
      tlsConfig:
        insecureSkipVerify: true
      bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token


Apply it:

kubectl apply -f kubelet-servicemonitor.yaml

🧾 5️⃣ Verify Prometheus Picks It Up

After ~30 seconds, open Prometheus UI →
Status → Targets and look for:

job="kubelet"
job="kubelet-cadvisor"


ou already have these metrics (since kubelet + cAdvisor targets are UP)

Now that you have both kubelet and kubelet-cadvisor targets visible and UP in Prometheus,
you already have per-pod and per-container resource data.

Try this in Prometheus → Graph → Execute:

Per-container CPU usage
rate(container_cpu_usage_seconds_total[2m])

Per-pod CPU usage
sum by (pod, namespace) (rate(container_cpu_usage_seconds_total[2m]))

Per-pod memory usage
sum by (pod, namespace) (container_memory_usage_bytes)

Top 5 CPU-consuming pods
topk(5, sum by (pod, namespace) (rate(container_cpu_usage_seconds_total[2m])))


✅ You’ll see real-time data for every container running in the cluster — even your café Node.js app.



kubectl run badpod --image=nonexistent:latest  

kubectl run crashloop-demo --image=busybox   --restart=Always --command -- sh -c "echo 'Simulating CrashLoop'; sleep 2; exit 1" 
   
  
  

kubectl get prometheusrules -l release=prometheus

kubectl get prometheus -o yaml | grep -A5 ruleSelector

kubectl get prometheus -o yaml | grep -A5 alerting

kubectl get pods -l alertmanager=alertmanager

kubectl describe pod -l alertmanager=alertmanager

kubectl get secret alertmanager-alertmanager -o jsonpath='{.data.alertmanager\.yaml}' | base64 -d

kubectl logs -l alertmanager=alertmanager -c alertmanager 

kubectl exec -it $(kubectl get pod -l prometheus=prometheus -o name) -- prometheus --version

kubectl exec -it $(kubectl get pod -l alertmanager=alertmanager -o name) -- alertmanager --version  



####Helm to install prometheus Monitoring Stack#####


helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
helm install prometheus prometheus-community/kube-prometheus-stack


