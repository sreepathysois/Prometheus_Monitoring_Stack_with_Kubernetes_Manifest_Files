###### Intsall prometheus operator and all CRD"""


kubectl create -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/master/bundle.yaml


####create file called prom_rbac.yam#########

# prom_rbac.yaml (fixed)
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
rules:
- apiGroups: [""]
  resources:
  - nodes
  - nodes/metrics
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["get"]
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  verbs: ["get", "list", "watch"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: default



kubectl apply -f prom_rbac.yaml  



#####crete prometheus statefullset resource prometheus.yaml####



apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: prometheus
  labels:
    app: prometheus
spec:
  image: quay.io/prometheus/prometheus:v2.55.1
  nodeSelector:
    kubernetes.io/os: linux
  replicas: 2
  resources:
    requests:
      memory: 400Mi
  securityContext:
    fsGroup: 2000
    runAsNonRoot: true
    runAsUser: 1000
  serviceAccountName: prometheus
  version: v2.22.1
  serviceMonitorSelector: {}
  serviceMonitorNamespaceSelector: {}
  ruleSelector: {}
  ruleNamespaceSelector: {}
  alerting:
    alertmanagers:
      - namespace: default
        name: alertmanager
        port: web

kubectl apply -f prometheus.yaml 



####create prometheus service prometheus_svc.yaml####



apiVersion: v1
kind: Service
metadata:
  name: prometheus
  labels:
    app: prometheus
spec:
  ports:
  - name: web
    port: 9090
    targetPort: web
  selector:
    app.kubernetes.io/name: prometheus
  sessionAffinity: ClientIP


kubectl apply -f prometheus_svc.yaml


####Test application Exposing Metrics###########


sudo git clone https://github.com/sreepathysois/Nodej_Prometheus_Metric-Monitor.git

cd Nodej_Prometheus_Metric-Monitor


kubectl apply -f nodejs_prometheus_deployment.yaml

kubetcl apply -f nodejs_prometheus_service.yaml
 
kubetcl apply -f nodejs_prometheus_service_monitor.yaml  




Node.js App  ‚Üí  Service  ‚Üí  ServiceMonitor
     ‚Üì                    ‚Üë
  exports /metrics         \
       ‚Üì                    \
     Prometheus ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Prometheus Operator
       ‚Üì
   Grafana (optional) 



####Deploy grafana grafana_deploy_svc.yaml#########



apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      containers:
        - name: grafana
          image: grafana/grafana:latest
          env:
            - name: GF_SECURITY_ADMIN_USER
              value: admin
            - name: GF_SECURITY_ADMIN_PASSWORD
              value: admin
          ports:
            - containerPort: 3000
              name: web
---
apiVersion: v1
kind: Service
metadata:
  name: grafana
spec:
  type: NodePort
  selector:
    app: grafana
  ports:
    - port: 3000
      targetPort: 3000
      nodePort: 30030
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-datasource
  labels:
    grafana_datasource: "1"
data:
  prometheus-datasource.yaml: |
    apiVersion: 1
    datasources:
      - name: Prometheus
        type: prometheus
        access: proxy
        url: http://prometheus:9090
        isDefault: true


kubetcl apply -f grafana_deploy_svc.yaml


####Prometheus pql####




üß† Understanding Metric Types You Have

From your output, your app exports:

Metric Name	Type	Description
cafe_welcome_requests_total	counter	total number of welcome page hits
process_cpu_seconds_total	counter	total CPU time used (user+system)
process_resident_memory_bytes	gauge	current memory usage
nodejs_eventloop_lag_seconds	gauge	event loop lag (main thread delay)
nodejs_heap_size_used_bytes	gauge	heap memory currently used
nodejs_gc_duration_seconds	histogram	GC pause durations by type
nodejs_active_handles_total	gauge	active I/O handles
nodejs_active_resources_total	gauge	active event loop resources
nodejs_version_info	gauge	Node.js version metadata

Let‚Äôs query these step by step. üëá

‚öôÔ∏è 1Ô∏è‚É£ Request Rate ‚Äî How many welcome requests per second?
rate(cafe_welcome_requests_total[1m])


üëâ Returns the requests per second over the last minute.
This is the most common way to track web endpoint traffic.

‚úÖ In Grafana, you can visualize this as ‚ÄúRequests per second.‚Äù

You can also take a longer window (5m average):

rate(cafe_welcome_requests_total[5m])

‚öôÔ∏è 2Ô∏è‚É£ Total Request Count
cafe_welcome_requests_total


This just shows the total number (e.g., 3).
Useful for verifying that your counter increments as you refresh the web page.

‚öôÔ∏è 3Ô∏è‚É£ CPU Usage (seconds per second ‚Üí percentage)

Prometheus exposes CPU time as a counter, so you need to rate it:

rate(process_cpu_seconds_total[1m])


Multiply by 100 for approximate CPU utilization (% of one core):

rate(process_cpu_seconds_total[1m]) * 100

‚öôÔ∏è 4Ô∏è‚É£ Memory Usage (Resident / Heap)

Resident (RSS):

process_resident_memory_bytes


This shows total physical memory used (bytes).

Convert to MB:

process_resident_memory_bytes / 1024 / 1024


Heap Memory Used:

nodejs_heap_size_used_bytes / 1024 / 1024

‚öôÔ∏è 5Ô∏è‚É£ Event Loop Lag ‚Äî responsiveness indicator
nodejs_eventloop_lag_seconds


This tells you how blocked your Node.js main thread is.
Healthy apps usually stay below 0.02s (20ms).

‚öôÔ∏è 6Ô∏è‚É£ Number of Active Handles / Resources

Handles:

nodejs_active_handles_total


Resources:

nodejs_active_resources_total


To see resource breakdown (by type):

nodejs_active_resources

‚öôÔ∏è 7Ô∏è‚É£ Heap Space Usage by Space

For detailed breakdown (old, new, code, etc.):

Used bytes per heap space:

nodejs_heap_space_size_used_bytes


Percentage of heap used per space:

(nodejs_heap_space_size_used_bytes / nodejs_heap_space_size_total_bytes) * 100

‚öôÔ∏è 8Ô∏è‚É£ Garbage Collection Metrics (Histogram)

Total GC count by type:

sum(nodejs_gc_duration_seconds_count) by (kind)


Total GC time spent (seconds):

sum(nodejs_gc_duration_seconds_sum) by (kind)


Average GC duration per GC type:

(sum(nodejs_gc_duration_seconds_sum) by (kind))
/
(sum(nodejs_gc_duration_seconds_count) by (kind))

‚öôÔ∏è 9Ô∏è‚É£ Node.js Version
nodejs_version_info


‚Üí returns {version="v18.20.8", major="18", minor="20", patch="8"} 1
A nice sanity check that metrics are coming from your running Node.js version.

‚öôÔ∏è üîü Combine Metrics for Dashboards

Dashboard ideas for Grafana:

Chart	PromQL Query	Notes
Requests per second	rate(cafe_welcome_requests_total[1m])	Traffic graph
CPU usage %	rate(process_cpu_seconds_total[1m]) * 100	CPU load
Memory (MB)	process_resident_memory_bytes / 1024 / 1024	RAM used
Heap usage %	(nodejs_heap_size_used_bytes / nodejs_heap_size_total_bytes) * 100	Memory efficiency
Event loop lag	nodejs_eventloop_lag_seconds	Responsiveness
Active handles	nodejs_active_handles_total	Connections / async handles
GC avg duration	(sum(nodejs_gc_duration_seconds_sum) by (kind)) / (sum(nodejs_gc_duration_seconds_count) by (kind))  




#### Alert Manager and Alerts Setup#####



####Alertmanager rbac create file alertmanager_rbac.yaml####



# alertmanager-rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: alertmanager
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: alertmanager
rules:
- apiGroups: [""]
  resources:
  - configmaps
  - secrets
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: alertmanager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: alertmanager
subjects:
- kind: ServiceAccount
  name: alertmanager
  namespace: default



kubectl apply -f alertmanager_rbac.yaml



####create alert manager deploy file alertmanager_deploy.yaml #####



# alertmanager.yaml
apiVersion: monitoring.coreos.com/v1
kind: Alertmanager
metadata:
  name: alertmanager
  labels:
    app: alertmanager
spec:
  replicas: 1
  serviceAccountName: alertmanager
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000



kubectl apply -f alertmanager_deploy.yaml  



#### Create alert manager service alertmanager-svc.yam####


# alertmanager-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: alertmanager
  labels:
    app: alertmanager
spec:
  type: NodePort
  selector:
    alertmanager: alertmanager
  ports:
    - name: web
      port: 9093
      targetPort: 9093
      nodePort: 30093


kubectl apply -f alertmanager_svc.yaml


#### Create Alert Rules and Config Files and Test###########

### Create prometheus_alert_rules.yaml


apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: kubernetes-ai-alerts
  namespace: default
  labels:
    app: prometheus
    release: prometheus
spec:
  groups:
  - name: ai-kubernetes-alerts
    rules:
    - alert: PodImagePullError
      expr: kube_pod_container_status_waiting_reason{reason=~"ImagePullBackOff|ErrImagePull"} > 0
      for: 15s
      labels:
        severity: warning
        ai_alert: "true"
      annotations:
        summary: "Pod image pull error detected"
        description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} failed to pull image. Verify image name, tag, or registry credentials."

    - alert: CrashLoopBackOff
      expr: kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff"} > 0
      for: 15s
      labels:
        severity: warning
        ai_alert: "true"
      annotations:
        summary: "Pod in CrashLoopBackOff state"
        description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is restarting repeatedly. Check logs and resource limits."

    - alert: PodOOMKilled
      expr: kube_pod_container_status_terminated_reason{reason="OOMKilled"} > 0
      for: 1m
      labels:
        severity: warning
        ai_alert: "true"
      annotations:
        summary: "Container OOMKilled"
        description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} was OOMKilled. Consider increasing memory limits."

    - alert: HighCpuUsage
      expr: sum(rate(container_cpu_usage_seconds_total{image!=""}[2m])) by (pod, namespace) > 0.8
      for: 2m
      labels:
        severity: warning
        ai_alert: "true"
      annotations:
        summary: "High CPU usage detected"
        description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is using >80% CPU for 2 minutes."

    - alert: HighMemoryUsage
      expr: sum(container_memory_usage_bytes{image!=""}) by (pod, namespace)
        / sum(kube_pod_container_resource_limits_memory_bytes{image!=""}) by (pod, namespace) > 0.9
      for: 2m
      labels:
        severity: warning
        ai_alert: "true"
      annotations:
        summary: "High memory usage detected"
        description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is using >90% of its memory limit."

    - alert: NodeDown
      expr: up{job="node-exporter"} == 0
      for: 1m
      labels:
        severity: critical
        ai_alert: "true"
      annotations:
        summary: "Node down"
        description: "Node {{ $labels.instance }} is not responding."

    - alert: PodNetworkUnavailable
      expr: kube_pod_status_reason{reason="NetworkUnavailable"} > 0
      for: 1m
      labels:
        severity: warning
        ai_alert: "true"
      annotations:
        summary: "Pod network unavailable"
        description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has network connectivity issues."

    - alert: NodeDiskPressure
      expr: kube_node_status_condition{condition="DiskPressure", status="true"} == 1
      for: 2m
      labels:
        severity: warning
        ai_alert: "true"
      annotations:
        summary: "Node disk pressure"
        description: "Node {{ $labels.node }} has DiskPressure=true. Check disk space or I/O bottlenecks."

    - alert: PodPending
      expr: kube_pod_status_phase{phase="Pending"} > 0
      for: 3m
      labels:
        severity: warning
        ai_alert: "true"
      annotations:
        summary: "Pod pending too long"
        description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} pending >3m. Possible scheduling/resource issues."

    - alert: PodUnschedulable
      expr: kube_pod_status_unschedulable > 0
      for: 2m
      labels:
        severity: warning
        ai_alert: "true"
      annotations:
        summary: "Pod unschedulable"
        description: "Pod {{ $labels.pod }} cannot be scheduled due to resource constraints."


kubectl apply -f prometheus_alert_rules.yam



####create alert manager config  alertmanager_secret_config.yaml#####


# alertmanager_secret_config.yaml
apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-alertmanager
  namespace: default
  labels:
    app: alertmanager
type: Opaque
stringData:
  alertmanager.yaml: |-
    global:
      resolve_timeout: 5m

    route:
      group_by: ['alertname']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 3h
      receiver: 'discord'
      routes:
        - match:
            ai_alert: "true"
          receiver: 'discord'
        - match_re:
            severity: "critical|warning"
          receiver: 'discord'

    receivers:
      - name: 'discord'
        webhook_configs:
          - url: 'https://discord.com/api/webhooks/1234567890/abcdEfGhIjKlMnOpQrStUvWxYz'
            send_resolved: true

    templates:
      - '/etc/alertmanager/config/*.tmpl'



#### Install exportes to export metrucs kube state metrics, node exporter####


### create kube_state_metrics.yaml####



# kube-state-metrics.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kube-state-metrics
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: kube-state-metrics
rules:
  - apiGroups: [""]
    resources: ["pods", "nodes", "services", "endpoints", "daemonsets", "replicasets", "statefulsets"]
    verbs: ["list", "watch"]
  - apiGroups: ["apps"]
    resources: ["deployments", "daemonsets", "statefulsets", "replicasets"]
    verbs: ["list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kube-state-metrics
subjects:
  - kind: ServiceAccount
    name: kube-state-metrics
    namespace: default
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kube-state-metrics
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kube-state-metrics
  template:
    metadata:
      labels:
        app: kube-state-metrics
    spec:
      serviceAccountName: kube-state-metrics
      containers:
        - name: kube-state-metrics
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0
          ports:
            - name: metrics
              containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: kube-state-metrics
  labels:
    app: kube-state-metrics
spec:
  type: ClusterIP
  selector:
    app: kube-state-metrics
  ports:
    - name: metrics
      port: 8080
      targetPort: 8080


#### create node_exporter.yaml###


# node-exporter.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: node-exporter
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: node-exporter
rules:
  - apiGroups: [""]
    resources: ["nodes", "nodes/proxy", "services", "endpoints", "pods"]
    verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: node-exporter
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: node-exporter
subjects:
  - kind: ServiceAccount
    name: node-exporter
    namespace: default
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
spec:
  selector:
    matchLabels:
      app: node-exporter
  template:
    metadata:
      labels:
        app: node-exporter
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9100"
    spec:
      serviceAccountName: node-exporter
      hostNetwork: true
      hostPID: true
      containers:
        - name: node-exporter
          image: prom/node-exporter:latest
          ports:
            - name: metrics
              containerPort: 9100
          args:
            - --path.sysfs=/host/sys
            - --path.rootfs=/host/root
          volumeMounts:
            - name: sys
              mountPath: /host/sys
              readOnly: true
            - name: root
              mountPath: /host/root
              readOnly: true
      volumes:
        - name: sys
          hostPath:
            path: /sys
        - name: root
          hostPath:
            path: /
---
apiVersion: v1
kind: Service
metadata:
  name: node-exporter
  labels:
    app: node-exporter
spec:
  type: ClusterIP
  selector:
    app: node-exporter
  ports:
    - name: metrics
      port: 9100
      targetPort: 9100


#### Create Service Monitors for Kube state metrics and Node exporter metrics

### Create kube_state_metrics_service_monotor.yaml


apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: kube-state-metrics
  labels:
    release: prometheus
spec:
  selector:
    matchLabels:
      app: kube-state-metrics
  endpoints:
    - port: metrics
      interval: 15s


#### create node_exporter_service_monitor.yaml

apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: node-exporter
  labels:
    release: prometheus
spec:
  selector:
    matchLabels:
      app: node-exporter
  endpoints:
    - port: metrics
      interval: 15s





##### Pod Monitor#####


Create a Kubernetes Service for Kubelet endpoints

We need a Service that points to all node kubelets.
Let‚Äôs make it in the kube-system namespace:

# kubelet-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubelet
  namespace: kube-system
  labels:
    k8s-app: kubelet
spec:
  selector:
    component: kubelet
  ports:
    - name: https-metrics
      port: 10250
      targetPort: 10250


üëâ This service helps Prometheus discover the kubelets on each node via Kubernetes API service discovery.

Apply it:

kubectl apply -f kubelet-service.yaml

‚öôÔ∏è 4Ô∏è‚É£ Create a ServiceMonitor for kubelet + cAdvisor

Now add this ServiceMonitor so Prometheus can scrape /metrics and /metrics/cadvisor endpoints.

# kubelet-servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: kubelet
  namespace: kube-system
  labels:
    release: prometheus
spec:
  namespaceSelector:
    matchNames:
      - kube-system
  selector:
    matchLabels:
      k8s-app: kubelet
  endpoints:
    - port: https-metrics
      scheme: https
      path: /metrics
      interval: 30s
      tlsConfig:
        insecureSkipVerify: true
      bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    - port: https-metrics
      scheme: https
      path: /metrics/cadvisor
      interval: 30s
      tlsConfig:
        insecureSkipVerify: true
      bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token


Apply it:

kubectl apply -f kubelet-servicemonitor.yaml

üßæ 5Ô∏è‚É£ Verify Prometheus Picks It Up

After ~30 seconds, open Prometheus UI ‚Üí
Status ‚Üí Targets and look for:

job="kubelet"
job="kubelet-cadvisor"


ou already have these metrics (since kubelet + cAdvisor targets are UP)

Now that you have both kubelet and kubelet-cadvisor targets visible and UP in Prometheus,
you already have per-pod and per-container resource data.

Try this in Prometheus ‚Üí Graph ‚Üí Execute:

Per-container CPU usage
rate(container_cpu_usage_seconds_total[2m])

Per-pod CPU usage
sum by (pod, namespace) (rate(container_cpu_usage_seconds_total[2m]))

Per-pod memory usage
sum by (pod, namespace) (container_memory_usage_bytes)

Top 5 CPU-consuming pods
topk(5, sum by (pod, namespace) (rate(container_cpu_usage_seconds_total[2m])))


‚úÖ You‚Äôll see real-time data for every container running in the cluster ‚Äî even your caf√© Node.js app.



kubectl run badpod --image=nonexistent:latest  

kubectl run crashloop-demo --image=busybox   --restart=Always --command -- sh -c "echo 'Simulating CrashLoop'; sleep 2; exit 1" 
   
  
  

kubectl get prometheusrules -l release=prometheus

kubectl get prometheus -o yaml | grep -A5 ruleSelector

kubectl get prometheus -o yaml | grep -A5 alerting

kubectl get pods -l alertmanager=alertmanager

kubectl describe pod -l alertmanager=alertmanager

kubectl get secret alertmanager-alertmanager -o jsonpath='{.data.alertmanager\.yaml}' | base64 -d

kubectl logs -l alertmanager=alertmanager -c alertmanager 

kubectl exec -it $(kubectl get pod -l prometheus=prometheus -o name) -- prometheus --version

kubectl exec -it $(kubectl get pod -l alertmanager=alertmanager -o name) -- alertmanager --version  



####Helm to install prometheus Monitoring Stack#####


helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
helm install prometheus prometheus-community/kube-prometheus-stack


