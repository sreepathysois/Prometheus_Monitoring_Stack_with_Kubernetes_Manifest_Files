# Kubernetes Monitoring Stack ‚Äî Prometheus, Alertmanager & Grafana

This project demonstrates a complete end-to-end Kubernetes monitoring and alerting setup using Prometheus Operator, Alertmanager, Grafana, and various exporters.
It also includes custom alerts, Discord integration, and application-level monitoring for a Node.js app exposing Prometheus metrics.

## üìò Overview

The monitoring architecture includes the following key components:

* Prometheus Operator ‚Üí Manages all Prometheus and Alertmanager custom resources.

* Prometheus ‚Üí Scrapes metrics from nodes, pods, and applications.

* Alertmanager ‚Üí Handles alert routing and sends notifications to Discord.

* Grafana ‚Üí Visualizes metrics via dashboards.

* Node Exporter ‚Üí Exports host-level metrics (CPU, memory, disk).

* Kube-State-Metrics ‚Üí Exposes cluster object states.

* Kubelet + cAdvisor ‚Üí Provides per-pod and per-container resource metrics.

* Node.js Demo App ‚Üí Sample web app exporting Prometheus-compatible metrics.

## üèóÔ∏è Architecture Diagram   


               
          

            
## üß± Step-by-Step Setup
### 1Ô∏è‚É£ Install Prometheus Operator & CRDs

Objective:
* Deploy the Prometheus Operator and its required CRDs to manage all Prometheus, Alertmanager, and related custom resources.

Command:

* kubectl create -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/master/bundle.yaml

### 2Ô∏è‚É£ Configure RBAC for Prometheus

Objective:
* Grant Prometheus permissions to access nodes, services, and pods for metric collection.
* Include Service Account, Cluster Role definition and Role Bindings. 

Command:
* kubectl apply -f prom_rbac.yaml

### 3Ô∏è‚É£ Deploy Prometheus StatefulSet

Objective:
Deploy Prometheus as a StatefulSet with persistent data storage and Operator-managed lifecycle.

Command:
kubectl apply -f prometheus.yaml

### 4Ô∏è‚É£ Create Prometheus Service

Objective:
* Expose Prometheus internally for service discovery and scraping.

Command:
* kubectl apply -f prometheus_svc.yaml

### 5Ô∏è‚É£ Deploy Node.js Application with Metrics

Objective:
* Deploy a sample Node.js app that exposes /metrics endpoint for Prometheus scraping via ServiceMonitor.

Commands:

* kubectl apply -f nodejs_prometheus_deployment.yaml

* kubetcl apply -f nodejs_prometheus_service.yaml
 
* kubetcl apply -f nodejs_prometheus_service_monitor.yaml  


* Verify the app exposes Prometheus metrics such as request rate, CPU, memory, heap, GC stats, etc.

### 6Ô∏è‚É£ Deploy Grafana

Objective:
* Install Grafana for visualizing Prometheus metrics through dashboards.

Command:
* kubectl apply -f grafana_deploy_svc.yaml

Outcome:
* Grafana UI accessible at NodeIP:30030, pre-configured with Prometheus as data source.

### 7Ô∏è‚É£ Deploy Alertmanager & RBAC

Objective:
* Set up Alertmanager for alert routing and integration with Discord.
* Grant required permissions via RBAC.

Commands:

* kubectl apply -f alertmanager_rbac.yaml

* kubectl apply -f alertmanager_deploy.yaml

* kubectl apply -f alertmanager_svc.yaml

### 8Ô∏è‚É£ Configure Alertmanager for Discord Notifications

Objective:
* Create a Kubernetes Secret containing the Alertmanager configuration to send alerts to a Discord webhook.

Command:
* kubectl apply -f alertmanager_secret_config.yaml

Result:
* Alertmanager routes alerts from Prometheus to Discord channels based on severity and labels.

### 9Ô∏è‚É£ Define Prometheus Alert Rules

Objective:
* Define PrometheusRule CRD to detect system and application issues such as:

* CrashLoopBackOff

* ImagePullBackOff

* OOMKilled

*High CPU / Memory

* Node Down

* Network Unavailable

* Disk Pressure

Command:
* kubectl apply -f prometheus_alert_rules.yaml

Result:
* Alerts visible under Prometheus ‚Üí Alerts, firing when thresholds are met and sent to Discord.

### üîü Deploy Exporters
A. Kube-State-Metrics

Objective:
Export cluster object state metrics (Deployments, Pods, StatefulSets, etc.) to Prometheus.

Command:
* kubectl apply -f kube_state_metrics.yaml

B. Node Exporter

Objective:
Expose node-level metrics (CPU, RAM, filesystem) using a DaemonSet.

Command:

* kubectl apply -f node_exporter.yaml

### 1Ô∏è‚É£1Ô∏è‚É£ Configure ServiceMonitors for Exporters

Objective:
Tell Prometheus Operator to scrape metrics from Kube-State-Metrics and Node Exporter services.

Commands:

* kubectl apply -f kube_state_metrics_service_monitor.yaml

* kubectl apply -f node_exporter_service_monitor.yaml

Result:
* Prometheus now collects node and cluster-level metrics.

### 1Ô∏è‚É£2Ô∏è‚É£ Enable Kubelet and cAdvisor Metrics

Objective:
Scrape kubelet /metrics and /metrics/cadvisor for per-pod and per-container CPU/memory data.

Commands:

* kubectl apply -f kubelet-service.yaml

* kubectl apply -f kubelet-servicemonitor.yaml

Result:
* Prometheus discovers new kubelet and kubelet-cadvisor targets.
* All pod and container metrics (CPU, memory, I/O) become visible.

### 1Ô∏è‚É£3Ô∏è‚É£ Validate Metrics & Alerts

Objectives:

* View targets via Prometheus ‚Üí Status ‚Üí Targets

* Run PromQL queries for:

* Per-pod CPU / Memory usage

* Top resource-consuming pods

* Simulate test alerts (CrashLoopBackOff, ImagePullBackOff)

Example Commands:
* kubectl run badpod --image=nonexistent:latest
* kubectl run crashloop-demo --image=busybox --restart=Always --command -- sh -c "sleep 2; exit 1"

Result:
* Prometheus alerts trigger and notifications sent to Discord.

### 1Ô∏è‚É£4Ô∏è‚É£ Verify Setup

Check:

* kubectl get prometheusrules

* kubectl get prometheus

* kubectl logs -l alertmanager=alertmanager

* kubectl exec -it $(kubectl get pod -l prometheus=prometheus -o name) -- prometheus --version

### 1Ô∏è‚É£5Ô∏è‚É£ (Optional) Install via Helm

Objective:
Install the entire monitoring stack using the official Helm chart to demonstrate GitOps or easier management.

Commands:

* helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
* helm repo update
* helm install prometheus prometheus-community/kube-prometheus-stack


Outcome:
Prometheus, Alertmanager, Grafana, exporters, and CRDs auto-installed and preconfigured.

1Ô∏è‚É£ Prometheus Operator Installed
       ‚Üì
2Ô∏è‚É£ Prometheus + Alertmanager CRDs Created
       ‚Üì
3Ô∏è‚É£ Exporters (Node, Kube-State, Kubelet) Deploy
       ‚Üì
4Ô∏è‚É£ ServiceMonitors ‚Üí Prometheus Discovers Targets
       ‚Üì
5Ô∏è‚É£ Prometheus Scrapes Metrics
       ‚Üì
6Ô∏è‚É£ PrometheusRules Evaluate Alerts
       ‚Üì
7Ô∏è‚É£ Alertmanager Routes Alerts ‚Üí Discord
       ‚Üì
8Ô∏è‚É£ Grafana Visualizes Metrics & Alerts


After completing all steps, your setup provides:

* Layer	Metric Source	Purpose
* Node-level	Node Exporter	CPU, Memory, Disk
* Cluster-level	Kube-State-Metrics	Pod, Deployment, ReplicaSet state
* Pod/Container-level	Kubelet + cAdvisor	Per-pod CPU, Memory, Network
* App-level	Node.js Metrics	Requests, Heap, Event Loop, GC
* Alerting	PrometheusRules + Alertmanager	Health & resource alerts
* Visualization	Grafana	Unified dashboards for all layers
### üìä Validation Checks

* Prometheus UI ‚Üí http://<NodeIP>:9090

* Grafana UI ‚Üí http://<NodeIP>:30030

* Alertmanager UI ‚Üí http://<NodeIP>:30093

* Discord Channel ‚Üí Alert notifications visible

### üåü Highlights

* End-to-end observability stack built from manifests.

* Works across any Kubernetes cluster.

* Fully integrated alerting via Discord.

* Modular ‚Äî can easily be replaced by Helm or GitOps.

* Extensible ‚Äî supports PodMonitor, custom exporters, and app metrics.
